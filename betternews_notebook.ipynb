{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocGN6D9Bwdrf"
      },
      "source": [
        "# **MACHINE LEARNING PROJECT EXAMS**\n",
        "\n",
        "### Name : Amanda Ofori\n",
        "### Course:  BSc. Computer Science\n",
        "### Roll Number: 10201100146\n",
        "\n",
        "\n",
        "## Question 2: **News Categorizing System**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing Packages**"
      ],
      "metadata": {
        "id": "bKttEa0IYDjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pCfNLIRCr3J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcZt3LcCwdsk"
      },
      "source": [
        "## **Setting up Directory to store models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9NvKsfcD_EL",
        "outputId": "3ea082a0-e06e-4e61-b1a7-13c0d72372ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: models\n"
          ]
        }
      ],
      "source": [
        "# Specify the directory name where you want to store the models\n",
        "models_dir = \"models\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "    print(f\"Created directory: {models_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {models_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the Dataset and Defining EDA and Preprocessing functions**"
      ],
      "metadata": {
        "id": "v73oRjHKrWs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgRy54jKXgBj",
        "outputId": "8bb09587-f7a6-45a5-a492-7b9a86745189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EDA on News Category dataset\n",
            "         category                                              title  \\\n",
            "0  ARTS & CULTURE  Modeling Agencies Enabled Sexual Predators For...   \n",
            "1  ARTS & CULTURE  Actor Jeff Hiller Talks âBright Colors And B...   \n",
            "2  ARTS & CULTURE  New Yorker Cover Puts Trump 'In The Hole' Afte...   \n",
            "3  ARTS & CULTURE  Man Surprises Girlfriend By Drawing Them In Di...   \n",
            "4  ARTS & CULTURE  This Artist Gives Renaissance-Style Sculptures...   \n",
            "\n",
            "                                                body  \n",
            "0  In October 2017, Carolyn Kramer received a dis...  \n",
            "1  This week I talked with actor Jeff Hiller abou...  \n",
            "2  The New Yorker is taking on President Donald T...  \n",
            "3  Kellen Hickey, a 26-year-old who lives in Huds...  \n",
            "4  Thereâs something about combining the tradit...  \n",
            "******************************\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6877 entries, 0 to 6876\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   category  6877 non-null   object\n",
            " 1   title     6877 non-null   object\n",
            " 2   body      6872 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 161.3+ KB\n",
            "None\n",
            "******************************\n",
            "              category                               title  \\\n",
            "count             6877                                6877   \n",
            "unique              14                                6836   \n",
            "top     ARTS & CULTURE  Extreme Weather Photos Of The Week   \n",
            "freq              1002                                  24   \n",
            "\n",
            "                                                     body  \n",
            "count                                                6872  \n",
            "unique                                               6815  \n",
            "top     This week brought several big headlines in ext...  \n",
            "freq                                                   21  \n",
            "******************************\n",
            "category    0\n",
            "title       0\n",
            "body        5\n",
            "dtype: int64\n",
            "Index(['category', 'title', 'body'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['category', 'title', 'body'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Load the data set\n",
        "data = pd.read_csv('news-article-categories.csv', encoding='latin-1')\n",
        "\n",
        "print(\"EDA on News Category dataset\")\n",
        "print(data.head())\n",
        "print(\"**\"*15)\n",
        "# Get a concise summary of the dataframe\n",
        "print(data.info())\n",
        "print(\"**\"*15)\n",
        "# Summary statistics for numeric columns\n",
        "print(data.describe())\n",
        "print(\"**\"*15)\n",
        "# Count the number of missing values in each column\n",
        "missing_values = data.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "\n",
        "# Fill NaN values in the 'body' column with empty strings\n",
        "data['body'] = data['body'].fillna('')\n",
        "print(data.columns)\n",
        "\n",
        "# Preprocessing of thr dataset\n",
        "\n",
        " # Ensure you've downloaded necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet') # For lemmatization\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define preprocessing functions\n",
        "def lemmatize_words(text):\n",
        "    wordnet = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([wordnet.lemmatize(word) for word in words])\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "def remove_single_characters_and_excessive_spaces(text):\n",
        "    # Remove single characters and excessive spaces\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters surrounded by spaces\n",
        "    text = re.sub(r'^[a-zA-Z]\\s+', ' ', text)    # Remove single character at the start followed by a space\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I) # Remove excessive spaces\n",
        "    return text.strip()  # Remove leading and trailing spaces\n",
        "\n",
        "def convert_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_special_chars(text):\n",
        "    text_without_special_chars = []\n",
        "    for char in text:\n",
        "        if char.isalnum():\n",
        "            text_without_special_chars.append(char)\n",
        "        else:\n",
        "            text_without_special_chars.append(' ')\n",
        "    return \"\".join(text_without_special_chars)\n",
        "\n",
        "def remove_tags(text):\n",
        "    tag_regex = re.compile(r'<[^>]+>')\n",
        "    return re.sub(tag_regex, '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word not in stop_words])\n",
        "\n",
        "def mode(results):\n",
        "    freqs = Counter(results)\n",
        "    ranking = sorted(freqs.items(), key=lambda pair: pair[1], reverse=True)\n",
        "    return ranking[0][0]  # This returns the most frequent category name directly\n",
        "\n",
        "\n",
        "# Define preprocessing functions\n",
        "def preprocess_article_text(article_text):\n",
        "    article_text = remove_tags(article_text)\n",
        "    article_text = remove_special_chars(article_text)\n",
        "    article_text = convert_lower(article_text)\n",
        "    article_text = remove_stopwords(article_text)\n",
        "    article_text = lemmatize_words(article_text)\n",
        "    return article_text\n",
        "\n",
        "\n",
        "# pre-process the text from the dataset\n",
        "data['body'] = data['body'].apply(preprocess_article_text)\n",
        "print(data.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the train models function**"
      ],
      "metadata": {
        "id": "jtflSehvwLdA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJXHoso9aH7y"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "def train_models():\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data['body'], data['category'], test_size=0.3, random_state=42)\n",
        "\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit the vectorizer on the training data and transform it\n",
        "    X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "    # Transform the test data using the same vectorizer\n",
        "    X_test_vectorized = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Save model\n",
        "    model_filename = os.path.join(models_dir, \"vectoriser.pkl\")\n",
        "    if not os.path.exists(model_filename):\n",
        "        with open(model_filename, 'wb') as f:\n",
        "            pickle.dump(tfidf_vectorizer, f)\n",
        "        print(f\"Vectorizer saved to: {model_filename}\")\n",
        "    else:\n",
        "        print(f\"Vectorizer file already exists: {model_filename}\")\n",
        "\n",
        "\n",
        "    # Define individual models\n",
        "    mnb = MultinomialNB()\n",
        "    rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
        "    xgb = XGBClassifier(n_estimators=50, random_state=2)\n",
        "\n",
        "    # Create a Voting Classifier\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[('mnb', mnb), ('rf', rfc), ('xgb', xgb)],\n",
        "        voting='soft'\n",
        "    )\n",
        "\n",
        "    # Training the Ensemble Model\n",
        "    voting_clf.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Hyperparameter Tuning for Random Forest\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X_train_vectorized, y_train)\n",
        "    rfc_best = grid_search.best_estimator_\n",
        "\n",
        "    # Update the Voting Classifier with the best Random Forest model\n",
        "    voting_clf.set_params(rf=rfc_best)\n",
        "\n",
        "    # Re-train the updated Ensemble Model\n",
        "    voting_clf.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Save the trained voting classifier to a file named 'voting_classifier.pkl'\n",
        "    model_filename = os.path.join(models_dir, \"voting_classifier.pkl\")\n",
        "    if not os.path.exists(model_filename):\n",
        "        with open(model_filename, 'wb') as f:\n",
        "            pickle.dump(voting_clf, f)\n",
        "            print(f\"Saved trained voting classifier to: {model_filename}\")\n",
        "    else:\n",
        "        print(f\"File already exists: {model_filename}. Skipping save operation.\")\n",
        "\n",
        "    # Model evaluation\n",
        "    y_pred = voting_clf.predict(X_test_vectorized)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    return voting_clf, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the Models for Prediction**"
      ],
      "metadata": {
        "id": "jVfaWNuPwZxy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGSaNNnJbqsN"
      },
      "outputs": [],
      "source": [
        "# Load models and vectorizer for prediction\n",
        "def load_models_and_vectorizer(models_path):\n",
        "    vectorizer = None\n",
        "    models = {}\n",
        "    for root, _, files in os.walk(models_path):\n",
        "        for file in files:\n",
        "            model_path = os.path.join(root, file)\n",
        "            with open(model_path, 'rb') as f:\n",
        "                if file == 'vectoriser.pkl':\n",
        "                    vectorizer = pickle.load(f)\n",
        "                else:\n",
        "                    model_name = file.split('.')[0]  # Extract model name from file name\n",
        "                    models[model_name] = pickle.load(f)\n",
        "    return vectorizer, models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the Make_Prediction function**"
      ],
      "metadata": {
        "id": "XU4YmTZqwycM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYgmGI_zb7CC"
      },
      "outputs": [],
      "source": [
        "# Make prediction\n",
        "def make_prediction(article_text):\n",
        "    categories = [\n",
        "        \"ARTS & CULTURE\",\n",
        "        \"BUSINESS\",\n",
        "        \"COMEDY\",\n",
        "        \"CRIME\",\n",
        "        \"EDUCATION\",\n",
        "        \"ENTERTAINMENT\",\n",
        "        \"ENVIRONMENT\",\n",
        "    ]\n",
        "    results = []\n",
        "    absolute_path = os.path.dirname(__file__)\n",
        "    models_path = os.path.join(absolute_path, 'models')\n",
        "    vectorizer, trained_models = load_models_and_vectorizer(models_path)\n",
        "\n",
        "    # Preprocess and vectorize the article text\n",
        "    processed_article_text = preprocess_article_text(article_text)\n",
        "    text_vectorized = vectorizer.transform([processed_article_text])\n",
        "\n",
        "    # Make predictions using the loaded models\n",
        "    for model_name, model in trained_models.items():\n",
        "        prediction = model.predict(text_vectorized)[0]\n",
        "        results.append(prediction)\n",
        "\n",
        "    # Determine the most common prediction\n",
        "    predicted_category = mode(results)\n",
        "    return predicted_category  # Return the predicted category directly\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}